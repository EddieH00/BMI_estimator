{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/eddieh00/UCSD/ms/ece228/bmi_model/facenet/src')\n",
    "import facenet\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prewhiten(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    std_adj = np.maximum(std, 1.0 / np.sqrt(x.size))\n",
    "    y = np.multiply(np.subtract(x, mean), 1 / std_adj)\n",
    "    return y\n",
    "\n",
    "def load_and_preprocess_image(image_path, image_size):\n",
    "    try:\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(image_size)\n",
    "        image = np.array(image)\n",
    "\n",
    "        # Preprocess the image\n",
    "        image = prewhiten(image)\n",
    "\n",
    "        return image\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found: \" + str(image_path))\n",
    "        return None\n",
    "\n",
    "def generate_embeddings(image_paths, labels, model_path, image_size):\n",
    "    assert len(image_paths) == len(labels), \"Number of image paths and labels must be the same.\"\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Load the facenet model\n",
    "            facenet.load_model(model_path)\n",
    "\n",
    "            # Get input and output tensors\n",
    "            input_tensor = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "            output_tensor = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "            phase_train_tensor = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "\n",
    "            # Generate embeddings and matching labels\n",
    "            embeddings = []\n",
    "            matching_labels = []\n",
    "            for image_path, label in zip(image_paths, labels):\n",
    "                image = load_and_preprocess_image(image_path, image_size)\n",
    "                if image is not None:\n",
    "                    image = np.expand_dims(image, axis=0)\n",
    "                    emb = sess.run(output_tensor, feed_dict={input_tensor: image, phase_train_tensor: False})\n",
    "                    embeddings.append(emb.flatten())\n",
    "                    matching_labels.append(label)\n",
    "\n",
    "            embeddings = np.array(embeddings)\n",
    "\n",
    "    return embeddings, matching_labels\n",
    "\n",
    "\n",
    "def generate_test_embeddings(image_paths, model_path, image_size):\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Load the facenet model\n",
    "            facenet.load_model(model_path)\n",
    "\n",
    "            # Get input and output tensors\n",
    "            input_tensor = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "            output_tensor = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "            phase_train_tensor = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "\n",
    "            # Generate embeddings and matching labels\n",
    "            embeddings = []\n",
    "            for image_path in image_paths:\n",
    "                image = load_and_preprocess_image(image_path, image_size)\n",
    "                if image is not None:\n",
    "                    image = np.expand_dims(image, axis=0)\n",
    "                    emb = sess.run(output_tensor, feed_dict={input_tensor: image, phase_train_tensor: False})\n",
    "                    embeddings.append(emb.flatten())\n",
    "\n",
    "            embeddings = np.array(embeddings)\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /home/eddieh00/UCSD/ms/ece228/bmi_model/facenet/best_facenet_model\n",
      "Metagraph file: model-20180402-114759.meta\n",
      "Checkpoint file: model-20180402-114759.ckpt-275\n",
      "INFO:tensorflow:Restoring parameters from /home/eddieh00/UCSD/ms/ece228/bmi_model/facenet/best_facenet_model/model-20180402-114759.ckpt-275\n",
      "File not found: /home/eddieh00/UCSD/ms/ece228/bmi_model/data/faces_precropped/_daewon_(lee_daewon 대원).jpg\n",
      "File not found: /home/eddieh00/UCSD/ms/ece228/bmi_model/data/faces_precropped/_damon_(qiū_báo_hàn).jpg\n",
      "File not found: /home/eddieh00/UCSD/ms/ece228/bmi_model/data/faces_precropped/_ha_bin_(ha_bin 하빈).jpg\n",
      "File not found: /home/eddieh00/UCSD/ms/ece228/bmi_model/data/faces_precropped/_harang_(park_ha_rang 하랑).jpg\n",
      "File not found: /home/eddieh00/UCSD/ms/ece228/bmi_model/data/faces_precropped/_manny_(xiào_dōngchéng).jpg\n",
      "File not found: /home/eddieh00/UCSD/ms/ece228/bmi_model/data/faces_precropped/_xin_(wáng_xīnyû).jpg\n",
      "File not found: /home/eddieh00/UCSD/ms/ece228/bmi_model/data/faces_precropped/aboutu_victor_han_(victor_han_baccic_galvão).jpg\n",
      "File not found: /home/eddieh00/UCSD/ms/ece228/bmi_model/data/faces_precropped/high4_alex_(alexsander_kim_알렉스_김)).jpg\n"
     ]
    }
   ],
   "source": [
    "from data_model import DataGenerator\n",
    "base_path = '/home/eddieh00/UCSD/ms/ece228/bmi_model/data/faces_precropped'\n",
    "csv_file = 'faces_precropped.csv'  # Replace with your CSV file name\n",
    "model_path = '/home/eddieh00/UCSD/ms/ece228/bmi_model/facenet/best_facenet_model'\n",
    "image_size = (160, 160)\n",
    "batch_size = 32\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(os.path.join(base_path, csv_file))\n",
    "image_paths = [os.path.join(base_path, filename) for filename in df['bookid']]\n",
    "labels = np.array(df['bmi'])\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings, labels = generate_embeddings(image_paths, labels, model_path, image_size)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the data into training, test, and validation sets\n",
    "train_embeddings, test_embeddings, train_labels, test_labels = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42)\n",
    "test_embeddings, val_embeddings, test_labels, val_labels = train_test_split(\n",
    "    test_embeddings, test_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create data loader objects\n",
    "train_data_loader = DataGenerator(train_embeddings, train_labels, batch_size)\n",
    "test_data_loader = DataGenerator(test_embeddings, test_labels, batch_size)\n",
    "val_data_loader = DataGenerator(val_embeddings, val_labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Average Loss 106.23621925354004\n",
      "Epoch 1 Average Loss 15.867789758046468\n",
      "Epoch 2 Average Loss 13.887044467926025\n",
      "Epoch 3 Average Loss 13.51119260152181\n",
      "Epoch 4 Average Loss 14.288290624618531\n",
      "Epoch 5 Average Loss 12.737537886301677\n",
      "Epoch 6 Average Loss 12.118235823313395\n",
      "Epoch 7 Average Loss 12.230643755594889\n",
      "Epoch 8 Average Loss 11.801600839296977\n",
      "Epoch 9 Average Loss 11.49229195912679\n",
      "Epoch 10 Average Loss 11.449333953857423\n",
      "Epoch 11 Average Loss 11.613825435638427\n",
      "Epoch 12 Average Loss 12.556619033813476\n",
      "Epoch 13 Average Loss 14.227193895975748\n",
      "Epoch 14 Average Loss 11.144591954549155\n",
      "Epoch 15 Average Loss 10.478056192398071\n",
      "Epoch 16 Average Loss 9.810902217229208\n",
      "Epoch 17 Average Loss 11.139845059712728\n",
      "Epoch 18 Average Loss 9.567644186019898\n",
      "Epoch 19 Average Loss 10.58549656867981\n"
     ]
    }
   ],
   "source": [
    "from bmiEstimator_model import BMI_Estimator\n",
    "model = BMI_Estimator()\n",
    "model.train(train_data_loader, 20, save=True, savePath='/home/eddieh00/UCSD/ms/ece228/bmi_model/bmi_model/saved models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/eddieh00/UCSD/ms/ece228/bmi_model/bmi_model/saved models/epoch_0/model_weights.ckpt\n"
     ]
    }
   ],
   "source": [
    "path = '/home/eddieh00/UCSD/ms/ece228/bmi_model/bmi_model/saved models/epoch_19/'\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    test = BMI_Estimator()\n",
    "        \n",
    "    checkpoint = tf.train.latest_checkpoint(path)\n",
    "    if checkpoint:\n",
    "        test.load(sess, checkpoint)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: /home/eddieh00/UCSD/ms/ece228/bmi_model/facenet/best_facenet_model\n",
      "Metagraph file: model-20180402-114759.meta\n",
      "Checkpoint file: model-20180402-114759.ckpt-275\n",
      "INFO:tensorflow:Restoring parameters from /home/eddieh00/UCSD/ms/ece228/bmi_model/facenet/best_facenet_model/model-20180402-114759.ckpt-275\n"
     ]
    }
   ],
   "source": [
    "test_image = '/home/eddieh00/UCSD/ms/ece228/bmi_model/data/test images/honh.png'\n",
    "facenet_path = '/home/eddieh00/UCSD/ms/ece228/bmi_model/facenet/best_facenet_model'\n",
    "testimg = generate_test_embeddings([test_image], model_path=facenet_path, image_size=(160, 160))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_4/BiasAdd_3:0\", shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "imgvector = testimg[0]\n",
    "imgtensor = tf.convert_to_tensor(imgvector, dtype=tf.float32)\n",
    "bmi = test.call(imgtensor)\n",
    "print(bmi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03407682]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output_value = sess.run(output_tensor)\n",
    "    print(output_value)\n",
    "path = '/home/eddieh00/UCSD/ms/ece228/bmi_model/bmi_model/saved models/epoch_0/'\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    test = BMI_Estimator()\n",
    "        \n",
    "    checkpoint = tf.train.latest_checkpoint(path)\n",
    "    if checkpoint:\n",
    "        test.load(sess, checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece228",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
